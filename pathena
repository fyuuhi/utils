#!/bin/bash

"exec" "python" "-u" "-Wignore" "$0" "$@"

import os
import re
import sys
import time
import copy
import shutil
import atexit
import commands
import optparse
import shelve
import datetime
import urllib
import random
import fcntl
import types
import traceback
import pickle

####################################################################

# error code
EC_Config    = 10
EC_CMT       = 20
EC_Extractor = 30
EC_Dataset   = 40
EC_Post      = 50
EC_Archive   = 60
EC_Split     = 70
EC_MyProxy   = 80
EC_Submit    = 90

#@ Number of events to skip in the file
nEventsToSkip=0
#@ Events blok counter per file
nSkips =0

# default cloud/site
defaultCloud = None

# max lookup for cross site option
maxCrossSite = 50

usage = """%prog [options] <jobOption1.py> [<jobOption2.py> [...]]

'%prog --help' prints a summary of the options

  HowTo is available at https://twiki.cern.ch/twiki/bin/view/PanDA/PandaAthena"""


# command-line parameters
optP = optparse.OptionParser(usage=usage,conflict_handler="resolve")
# special options
optP.add_option('--version',action='store_const',const=True,dest='version',default=False,
                help='Displays version')
optP.add_option('--split', action='store', dest='split',  default=-1,
                type='int',    help='Number of sub-jobs to be generated. This option is the same as --nJobs')
optP.add_option('--nJobs', action='store', dest='nJobs',  default=-1,
                type='int',    help='Number of sub-jobs to be generated. This option is the same as --split')
optP.add_option('--nFilesPerJob', action='store', dest='nFilesPerJob',  default=-1, type='int', help='Number of files on which each sub-job runs')
optP.add_option('--nEventsPerJob', action='store', dest='nEventsPerJob',  default=-1,
                type='int',    help='Number of events per subjob. This info is used mainly for job splitting. If you run on MC datasets, the total number of subjobs is nEventsPerFile*nFiles/nEventsPerJob. For data, the number of events for each file is retrieved from AMI and subjobs are created accordingly. Note that if you run transformations you need to explicitly specify maxEvents or something in --trf to set the number of events processed in each subjob. If you run normal jobOption files, evtMax and skipEvents in appMgr are automatically set on WN.')
optP.add_option('--nEventsPerFile', action='store', dest='nEventsPerFile',  default=0,
                type='int',    help='Number of events per file')
optP.add_option('--nGBPerJob',action='store',dest='nGBPerJob',default=-1, help='Instantiate one sub job per NGBPERJOB GB of input files. --nGBPerJob=MAX sets the size to the default maximum value')
optP.add_option('--nGBPerMergeJob',action='store',dest='nGBPerMergeJob',default=-1, help='Instantiate one merge job per NGBPERMERGEJOB GB of pre-merged files')
optP.add_option('--site', action='store', dest='site',  default="AUTO",
                type='string',    help='Site name where jobs are sent. If omitted, jobs are automatically sent to sites where input is available. A comma-separated list of sites can be specified (e.g. siteA,siteB,siteC), so that best sites are chosen from the given site list. If AUTO is appended at the end of the list (e.g. siteA,siteB,siteC,AUTO), jobs are sent to any sites if input is not found in the previous sites')
optP.add_option('--athenaTag',action='store',dest='athenaTag',default='',type='string',
                help='Use differnet version of Athena on remote WN. By defualt the same version which you are locally using is set up on WN. e.g., --athenaTag=AtlasProduction,14.2.24.3')
optP.add_option('--inDS',  action='store', dest='inDS',  default='',
                type='string', help='Input dataset names. wildcard and/or comma can be used to concatenate multiple datasets')
optP.add_option('--inDsTxt',action='store',dest='inDsTxt',default='',
                type='string', help='a text file which contains the list of datasets to run over. newlines are replaced by commas and the result is set to --inDS. lines starting with # are ignored')
optP.add_option('--minDS',  action='store', dest='minDS',  default='',
                type='string', help='Dataset name for minimum bias stream')
optP.add_option('--nMin',  action='store', dest='nMin',  default=-1,
                type='int', help='Number of minimum bias files per sub job')
optP.add_option('--lowMinDS',  action='store', dest='lowMinDS',  default='',
                type='string', help='Dataset name for low pT minimum bias stream')
optP.add_option('--nLowMin',  action='store', dest='nLowMin',  default=-1,
                type='int', help='Number of low pT minimum bias files per job')
optP.add_option('--highMinDS',  action='store', dest='highMinDS',  default='',
                type='string', help='Dataset name for high pT minimum bias stream')
optP.add_option('--nHighMin',  action='store', dest='nHighMin',  default=-1,
                type='int', help='Number of high pT minimum bias files per job')
optP.add_option('--randomMin',action='store_const',const=True,dest='randomMin',default=False,
                help='randomize files in minimum bias dataset')
optP.add_option('--cavDS',  action='store', dest='cavDS',  default='',
                type='string', help='Dataset name for cavern stream')
optP.add_option('--nCav',  action='store', dest='nCav',  default=-1,
                type='int', help='Number of cavern files per job')
optP.add_option('--randomCav',action='store_const',const=True,dest='randomCav',default=False,
                help='randomize files in cavern dataset')
optP.add_option('--libDS', action='store', dest='libDS', default='',
                type='string', help='Name of a library dataset')
optP.add_option('--goodRunListXML', action='store', dest='goodRunListXML', default='',
                type='string', help='Good Run List XML which will be converted to datasets by AMI')
optP.add_option('--goodRunListDataType', action='store', dest='goodRunDataType', default='',
                type='string', help='specify data type when converting Good Run List XML to datasets, e.g, AOD (default)')
optP.add_option('--goodRunListProdStep', action='store', dest='goodRunProdStep', default='',
                type='string', help='specify production step when converting Good Run List to datasets, e.g, merge (default)')
optP.add_option('--goodRunListDS', action='store', dest='goodRunListDS', default='',
                type='string', help='A comma-separated list of pattern strings. Datasets which are converted from Good Run List XML will be used when they match with one of the pattern strings. Either \ or "" is required when a wild-card is used. If this option is omitted all datasets will be used')
optP.add_option('--eventPickEvtList',action='store',dest='eventPickEvtList',default='',
                type='string', help='a file name which contains a list of runs/events for event picking')
optP.add_option('--eventPickDataType',action='store',dest='eventPickDataType',default='',
                type='string', help='type of data for event picking. one of AOD,ESD,RAW')
optP.add_option('--eventPickStreamName',action='store',dest='eventPickStreamName',default='',
                type='string', help='stream name for event picking. e.g., physics_CosmicCaloEM')
optP.add_option('--eventPickDS',action='store',dest='eventPickDS',default='',
                type='string', help='A comma-separated list of pattern strings. Datasets which are converted from the run/event list will be used when they match with one of the pattern strings. Either \ or "" is required when a wild-card is used. e.g., data\*')
optP.add_option('--eventPickStagedDS',action='store',dest='eventPickStagedDS',default='',
                type='string', help='--eventPick options create a temporary dataset to stage-in interesting files when those files are available only on TAPE, and then a stage-in request is automatically sent to DaTRI. Once DaTRI transfers the dataset to DISK you can use the dataset as an input using this option')
optP.add_option('--eventPickAmiTag',action='store',dest='eventPickAmiTag',default='',
                type='string', help='AMI tag used to match TAG collections names. This option is required when you are interested in older data than the latest one. Either \ or "" is required when a wild-card is used. e.g., f2\*')
optP.add_option('--eventPickNumSites',action='store',dest='eventPickNumSites',default=1,
                type='int', help='The event picking service makes a temprary dataset container to stage files to DISK. The consistuent datasets are distibued to N sites (N=1 by default)')
optP.add_option('--eventPickSkipDaTRI',action='store_const',const=True,dest='eventPickSkipDaTRI',default=False,
                help='Skip sending a staging request to DaTRI for event picking')
optP.add_option('--eventPickWithGUID',action='store_const',const=True,dest='eventPickWithGUID',default=False,
                help='Using GUIDs together with run and event numbers in eventPickEvtList to skip event lookup')
optP.add_option('--useNewTRF', action='store_const',const=True,dest='useNewTRF',default=True,
                help="Use the original filename with the attempt number for input in --trf when there is only one input, which follows the globbing scheme of new transformation framework")
optP.add_option('--useOldTRF', action='store_const',const=True,dest='useOldTRF',default=False,
                help="Remove the attempt number from the original filename for input in --trf when there is only one input")
optP.add_option('--useTagInTRF', action='store_const',const=True,dest='useTagInTRF',default=False,
                help="Set this option if you use TAG in --trf. If you run normal jobO this option is not required")
optP.add_option('--tagStreamRef',action='store',dest='tagStreamRef',default='',
                type='string', help='specify StreamRef of parent files when you use TAG in --trf. It must be one of StreamRAW,StreamESD,StreamAOD. E.g., if you want to read RAW files via TAGs, use --tagStreamRef=StreamRAW. If you run normal jobO, this option is ignored and EventSelector.RefName in your jobO is used')
optP.add_option('--tagQuery',action='store',dest='tagQuery',default='',
                type='string', help='specify Query for TAG preselection when you use TAG in --trf. If you run normal jobO, this option is ignored and EventSelector.Query in your jobO is used')
optP.add_option('--express', action='store_const',const=True,dest='express',default=False,
                help="Send the job using express quota to have higher priority. The number of express subjobs in the queue and the total execution time used by express subjobs are limited (a few subjobs and several hours per day, respectively). This option is intended to be used for quick tests before bulk submission. Note that buildXYZ is not included in quota calculation. If this option is used when quota has already exceeded, the panda server will ignore the option so that subjobs have normal priorities. Also, if you submit 1 buildXYZ and N runXYZ subjobs when you only have quota of M (M < N),  only the first M runXYZ subjobs will have higher priorities")
optP.add_option('--debugMode', action='store_const',const=True,dest='debugMode',default=False,
                help="Send the job with the debug mode on. If this option is specified the subjob will send stdout to the panda monitor every 5 min. The number of debug subjobs per user is limited. When this option is used and the quota has already exceeded, the panda server supresses the option so that subjobs will run without the debug mode. If you submit multiple subjobs in a single job, only the first subjob will set the debug mode on. Note that you can turn the debug mode on/off by using pbook after jobs are submitted" )
optP.add_option('--notUseTagLookup',action='store_const',const=True,dest='notUseTagLookup',default=False,
                help="don't use Event Lookup service to retrieve relation between TAG and parent datasets")
optP.add_option('--useContElementBoundary',action='store_const',const=True,dest='useContElementBoundary',default=False,
                help="Split job in such a way that sub jobs do not mix files of different datasets in the input container. See --useNthFieldForLFN too")
optP.add_option('--addNthFieldOfInDSToLFN',action='store',dest='addNthFieldOfInDSToLFN',default='',type='string',
                help="A middle name is added to LFNs of output files when they are produced from one dataset in the input container or input dataset list. The middle name is extracted from the dataset name. E.g., if --addNthFieldOfInDSToLFN=2 and the dataset name is data10_7TeV.00160387.physics_Muon..., 00160387 is extracted and LFN is something like user.hoge.TASKID.00160387.blah. Concatenate multiple field numbers with commas if necessary, e.g., --addNthFieldOfInDSToLFN=2,6.")
optP.add_option('--addNthFieldOfInFileToLFN',action='store',dest='addNthFieldOfInFileToLFN',default='',type='string',
                help="A middle name is added to LFNs of output files similarly as --addNthFieldOfInDSToLFN, but strings are extracted from input file names")
optP.add_option('--buildInLastChunk',action='store_const',const=True,dest='buildInLastChunk',default=False,
                help="Produce lib.tgz in the last chunk when jobs are split to multiple chunks due to the limit on the number of files in each chunk or due to --useContElementBoundary/--loadXML")
optP.add_option('--useAMIEventLevelSplit',action='store_const',const=True,dest='useAMIEventLevelSplit',default=None,
                help="retrive the number of events per file from AMI to split the job using --nEventsPerJob")
optP.add_option('--appendStrToExtStream',action='store_const',const=True,dest='appendStrToExtStream',default=False,
                help='append the first part of filenames to extra stream names for --individualOutDS. E.g., if this option is used together with --individualOutDS, %OUT.AOD.pool.root will be contained in an EXT0_AOD dataset instead of an EXT0 dataset')
optP.add_option('--mergeOutput', action='store_const', const=True, dest='mergeOutput', default=False,
                help="merge output files")
optP.add_option('--mergeScript',action='store',dest='mergeScript',default='',type='string',
                help='Specify user-defied script or execution string for output merging')
optP.add_option('--useCommonHalo', action='store_const', const=False, dest='useCommonHalo',  default=True,
                help="use an integrated DS for BeamHalo")
optP.add_option('--beamHaloDS',  action='store', dest='beamHaloDS',  default='',
                type='string', help='Dataset name for beam halo')
optP.add_option('--beamHaloADS',  action='store', dest='beamHaloADS',  default='',
                type='string', help='Dataset name for beam halo A-side')
optP.add_option('--beamHaloCDS',  action='store', dest='beamHaloCDS',  default='',
                type='string', help='Dataset name for beam halo C-side')
optP.add_option('--nBeamHalo',  action='store', dest='nBeamHalo',  default=-1,
                type='int', help='Number of beam halo files per sub job')
optP.add_option('--nBeamHaloA',  action='store', dest='nBeamHaloA',  default=-1,
                type='int', help='Number of beam halo files for A-side per sub job')
optP.add_option('--nBeamHaloC',  action='store', dest='nBeamHaloC',  default=-1,
                type='int', help='Number of beam halo files for C-side per sub job')
optP.add_option('--useCommonGas', action='store_const', const=False, dest='useCommonGas',  default=True,
                help="use an integrated DS for BeamGas")
optP.add_option('--beamGasDS',  action='store', dest='beamGasDS',  default='',
                type='string', help='Dataset name for beam gas')
optP.add_option('--beamGasHDS',  action='store', dest='beamGasHDS',  default='',
                type='string', help='Dataset name for beam gas Hydrogen')
optP.add_option('--beamGasCDS',  action='store', dest='beamGasCDS',  default='',
                type='string', help='Dataset name for beam gas Carbon')
optP.add_option('--beamGasODS',  action='store', dest='beamGasODS',  default='',
                type='string', help='Dataset name for beam gas Oxygen')
optP.add_option('--nBeamGas',  action='store', dest='nBeamGas',  default=-1,
                type='int', help='Number of beam gas files per sub job')
optP.add_option('--nBeamGasH',  action='store', dest='nBeamGasH',  default=-1,
                type='int', help='Number of beam gas files for Hydrogen per sub job')
optP.add_option('--nBeamGasC',  action='store', dest='nBeamGasC',  default=-1,
                type='int', help='Number of beam gas files for Carbon per sub job')
optP.add_option('--nBeamGasO',  action='store', dest='nBeamGasO',  default=-1,
                type='int', help='Number of beam gas files for Oxygen per sub job')
optP.add_option('--parentDS', action='store', dest='parentDS', default='',
                type='string', help='Parent dataset names. The brokerage takes their locations into account for TAG-based analysis')
optP.add_option('--outDS', action='store', dest='outDS', default='',
                type='string', help='Name of an output dataset. OUTDS will contain all output files')
optP.add_option('--destSE',action='store', dest='destSE',default='',
                type='string', help='Destination strorage element')
optP.add_option('--nFiles', '--nfiles', action='store', dest='nfiles',  default=0,
                type='int',    help='Use an limited number of files in the input dataset')
optP.add_option('--nSkipFiles', action='store', dest='nSkipFiles',  default=0,
                type='int',    help='Skip N files in the input dataset')
optP.add_option('--provenanceID',action='store',dest='provenanceID',default=-1,type='int',
                help='provenanceID')
optP.add_option('--useSiteGroup',action='store',dest='useSiteGroup',default=-1,type='int',
                help='Use only site groups which have group numbers not higher than --siteGroup. Group 0: T1 or undefined, 1,2,3,4: alpha,bravo,charlie,delta which are defined based on site reliability')
optP.add_option('-v', action='store_const', const=True, dest='verbose',  default=False,
                help='Verbose')
optP.add_option('-l', '--long', action='store_const', const=True, dest='long',  default=False,
                help='Send job to a long queue')
optP.add_option('--blong', action='store_const', const=True, dest='blong',  default=False,
                help='Send build job to a long queue')
optP.add_option('--noEmail', action='store_const', const=True, dest='noEmail',  default=False,
                help='Suppress email notification')
optP.add_option('--update', action='store_const', const=True, dest='update',  default=False,
                help='Update panda-client to the latest version')
optP.add_option('--cloud',action='store', dest='cloud',default=None,
                type='string', help='cloud where jobs are submitted. default is set according to your VOMS country group')
optP.add_option('--noBuild', action='store_const', const=True, dest='nobuild',  default=False,
                help='Skip buildJob')
optP.add_option('--noCompile', action='store_const',const=True,dest='noCompile',default=False,
                help='Just upload a tarball in the build step to avoid the tighter size limit imposed by --noBuild. The tarball contains binaries compiled on your local computer, so that compilation is skipped in the build step on remote WN')
optP.add_option('--noOutput', action='store_const', const=True, dest='noOutput',  default=False,
                help='Send job even if there is no output file')
optP.add_option('--individualOutDS', action='store_const', const=True, dest='individualOutDS',  default=False,
                help='Create individual output dataset for each data-type. By default, all output files are added to one output dataset')
optP.add_option('--transferredDS',action='store', dest='transferredDS',default='',type='string',
                help='Specify a comma-separated list of patterns so that only datasets which match the given patterns are transferred when --destSE is set. Either \ or "" is required when a wildcard is used. If omitted, all datasets are transferred')
optP.add_option('--noRandom', action='store_const', const=True, dest='norandom',  default=False,
                help='Enter random seeds manually')
optP.add_option('--useAMIAutoConf',action='store_const',const=True,dest='useAMIAutoConf',default=False,
                help='Use AMI for AutoConfiguration')
optP.add_option('--memory', action='store', dest='memory',  default=-1,
                type='int',    help='Required memory size in MB. e.g., for 1GB --memory 1024')
optP.add_option('--forceStaged', action='store_const', const=True, dest='forceStaged', default=False,
                help='Force files from primary DS to be staged to local disk, even if direct-access is possible')
optP.add_option('--maxCpuCount', action='store', dest='maxCpuCount', default=0, type='int',
                help='Required CPU count in seconds. Mainly to extend time limit for looping job detection')
optP.add_option('--official', action='store_const', const=True, dest='official',  default=False,
                help='Produce official dataset')
optP.add_option('--unlimitNumOutputs', action='store_const', const=True, dest='unlimitNumOutputs',  default=False,
                help='Remove the limit on the number of outputs. Note that having too many outputs per job causes a severe load on the system. You may be banned if you carelessly use this option') 
optP.add_option('--descriptionInLFN',action='store',dest='descriptionInLFN',default='',
                help='LFN is user.nickname.jobsetID.something (e.g. user.harumaki.12345.AOD._00001.pool) by default. This option allows users to put a description string into LFN. i.e., user.nickname.jobsetID.description.something')
optP.add_option('--extFile', action='store', dest='extFile',  default='',
                help='pathena exports files with some special extensions (.C, .dat, .py .xml) in the current directory. If you want to add other files, specify their names, e.g., data1.root,data2.doc')
optP.add_option('--excludeFile',action='store',dest='excludeFile',default='',
                help='specify a comma-separated string to exclude files and/or directories when gathering files in local working area. Either \ or "" is required when a wildcard is used. e.g., doc,\*.C')
optP.add_option('--extOutFile', action='store', dest='extOutFile',  default='',
                help='A comma-separated list of extra output files which cannot be extracted automatically. Either \ or "" is required when a wildcard is used. e.g., output1.txt,output2.dat,JiveXML_\*.xml')
optP.add_option('--supStream', action='store', dest='supStream',  default='',
                help='suppress some output streams. Either \ or "" is required when a wildcard is used. e.g., ESD,TAG,GLOBAL,StreamDESD\* ')
optP.add_option('--gluePackages', action='store', dest='gluePackages',  default='',
                help='list of glue packages which pathena cannot find due to empty i686-slc4-gcc34-opt. e.g., External/AtlasHepMC,External/Lhapdf')
optP.add_option('--allowNoOutput',action='store',dest='allowNoOutput',default='',type='string',
                help='A comma-separated list of regexp patterns. Output files are allowed not to be produced if their filenames match with one of regexp patterns. Jobs go to finished even if they are not produced on WN')
optP.add_option('--excludedSite', action='append', dest='excludedSite',  default=[],
                help="list of sites which are not used for site section, e.g., ANALY_ABC,ANALY_XYZ")
optP.add_option('--noSubmit', action='store_const', const=True, dest='nosubmit',  default=False,
                help="Don't submit jobs")
optP.add_option('--prodSourceLabel', action='store', dest='prodSourceLabel',  default='',
                help="set prodSourceLabel")
optP.add_option('--processingType', action='store', dest='processingType',  default='pathena',
                help="set processingType")
optP.add_option('--seriesLabel', action='store', dest='seriesLabel',  default='',
                help="set seriesLabel")
optP.add_option('--workingGroup', action='store', dest='workingGroup',  default=None,
                help="set workingGroup")
optP.add_option('--generalInput', action='store_const', const=True, dest='generalInput',  default=False,
                help='Read input files with general format except POOL,ROOT,ByteStream')
optP.add_option('--crossSite',action='store',dest='crossSite',default=maxCrossSite,
                type='int',help='submit jobs to N sites at most when datasets in container split over many sites (N=%s by default)' % maxCrossSite)
optP.add_option('--tmpDir', action='store', dest='tmpDir', default='',
                type='string', help='Temporary directory in which an archive file is created')
optP.add_option('--shipInput', action='store_const', const=True, dest='shipinput',  default=False,
                help='Ship input files to remote WNs')
optP.add_option('--noLock', action='store_const', const=True, dest='nolock',  default=False,
                help="Don't create a lock for local database access")
optP.add_option('--disableAutoRetry',action='store_const',const=True,dest='disableAutoRetry',default=False,
                help='disable automatic job retry on the server side')
optP.add_option('--fileList', action='store', dest='filelist', default='',
                type='string', help='List of files in the input dataset to be run')
optP.add_option('--myproxy', action='store', dest='myproxy', default='myproxy.cern.ch',
                type='string', help='Name of the myproxy server')
optP.add_option('--dbRelease', action='store', dest='dbRelease', default='',
                type='string', help='DBRelease or CDRelease (DatasetName:FileName). e.g., ddo.000001.Atlas.Ideal.DBRelease.v050101:DBRelease-5.1.1.tar.gz. If --dbRelease=LATEST, the latest DBRelease is used')
optP.add_option('--dbRunNumber', action='store', dest='dbRunNumber', default='',
                type='string', help='RunNumber for DBRelease or CDRelease. If this option is used some redundant files are removed to save disk usage when unpacking DBRelease tarball. e.g., 0091890')
optP.add_option('--addPoolFC', action='store', dest='addPoolFC',  default='',
                help="file names to be inserted into PoolFileCatalog.xml except input files. e.g., MyCalib1.root,MyGeom2.root") 
optP.add_option('--skipScan', action='store_const', const=True, dest='skipScan', default=False,
                help='Skip LFC lookup at job submission')
optP.add_option('--inputFileList', action='store', dest='inputFileList', default='',
                type='string', help='name of file which contains a list of files to be run in the input dataset')
optP.add_option('--removeFileList', action='store', dest='removeFileList', default='',
                type='string', help='name of file which contains a list of files to be removed from the input dataset')
optP.add_option('--removedDS', action='store', dest='removedDS', default='',
                type='string', help="don't use datasets in the input dataset container")
optP.add_option('--corCheck', action='store_const', const=True, dest='corCheck',  default=False,
                help='Enable a checker to skip corrupted files')
optP.add_option('--prestage', action='store_const', const=True, dest='prestage',  default=False,
                help='EXPERIMENTAL : Enable prestager. Make sure that you are authorized')
optP.add_option('--voms', action='store', dest='vomsRoles',  default=None, type='string',
                help="generate proxy with paticular roles. e.g., atlas:/atlas/ca/Role=production,atlas:/atlas/fr/Role=pilot")
optP.add_option('--useNextEvent', action='store_const', const=True, dest='useNextEvent',  default=False,
                help="Set this option if your jobO uses theApp.nextEvent(), e.g. for G4. Note that this option is not required when you run transformations using --trf")
optP.add_option('--ara', action='store_const', const=True, dest='ara',  default=False,
                help='obsolete. Please use prun instead')
optP.add_option('--ares', action='store_const', const=True, dest='ares',  default=False,
                help='obsolete. Please use prun instead')
optP.add_option('--araOutFile', action='store', dest='araOutFile',  default='',
                help='define output files for ARA, e.g., output1.root,output2.root')
optP.add_option('--trf', action='store', dest='trf',  default=False,
                help='run transformation, e.g. --trf "csc_atlfast_trf.py %IN %OUT.AOD.root %OUT.ntuple.root -1 0"')
optP.add_option('--spaceToken', action='store', dest='spaceToken', default='',
                type='string', help='spacetoken for outputs. e.g., ATLASLOCALGROUPDISK')
optP.add_option('--notSkipMissing', action='store_const', const=True, dest='notSkipMissing',  default=False,
                help='If input files are not read from SE, they will be skipped by default. This option disables the functionality')
optP.add_option('--burstSubmit', action='store', dest='burstSubmit', default='',
                type='string', help="Please don't use this option. Only for site validation by experts")
optP.add_option('--removeBurstLimit', action='store_const', const=True, dest='removeBurstLimit', default=False,
                help="Please don't use this option. Only for site validation by experts")
optP.add_option('--useShortLivedReplicas', action='store_const', const=True, dest='useShortLivedReplicas', default=False,
                help="Use replicas even if they have very sort lifetime")
optP.add_option('--useDirectIOSites', action='store_const', const=True, dest='useDirectIOSites', default=False,
                help="Use only sites which use directIO to read input files")
optP.add_option('--forceDirectIO', action='store_const', const=True, dest='forceDirectIO', default=False,
                help="Use directIO if directIO is available at the site ")
optP.add_option('--skipScout', action='store_const',const=True,dest='skipScout',default=False,
                help="skip scout jobs")
optP.add_option('--respectSplitRule', action='store_const',const=True,dest='respectSplitRule',default=False,
                help="force scout jobs to follow split rules like nGBPerJob")
optP.add_option('--devSrv', action='store_const', const=True, dest='devSrv',  default=False,
                help="Please don't use this option. Only for developers to use the dev panda server")
optP.add_option('--intrSrv', action='store_const', const=True, dest='intrSrv',  default=False,
                help="Please don't use this option. Only for developers to use the intr panda server")
optP.add_option('--useAIDA', action='store_const', const=True, dest='useAIDA',  default=False,
                help="use AIDA")
optP.add_option('--inputType', action='store', dest='inputType', default='',
                type='string', help='A regular expression pattern. Only files matching with the pattern in input dataset are used')
optP.add_option('--outTarBall', action='store', dest='outTarBall', default='',
                type='string', help='Save a gzipped tarball of local files which is the input to buildXYZ')
optP.add_option('--inTarBall', action='store', dest='inTarBall', default='',
                type='string', help='Use a gzipped tarball of local files as input to buildXYZ. Generall the tarball is created by using --outTarBall')
optP.add_option('--outRunConfig', action='store', dest='outRunConfig', default='',
                type='string', help='Save extracted config information to a local file')
optP.add_option('--inRunConfig', action='store', dest='inRunConfig', default='',
                type='string', help='Use a saved config information to skip config extraction')
optP.add_option('--mcData', action='store', dest='mcData', default='',
                type='string', help='Create a symlink with linkName to .dat which is contained in input file')
optP.add_option('--pfnList', action='store', dest='pfnList', default='',
                type='string', help='Name of file which contains a list of input PFNs. Those files can be un-registered in DDM')
optP.add_option('--outputPath',action='store',dest='outputPath', default='./',
                type='string', help='Physical path of output directory relative to a root path')
optP.add_option('--useExperimental', action='store_const', const=True, dest='useExperimental',  default=False,
                help='use experimental features')
optP.add_option('--useOldStyleOutput',action='store_const',const=True,dest='useOldStyleOutput',default=False,
                help="use output dataset and long LFN instead of output dataset container and short LFN (obsolete)")
optP.add_option('--disableRebrokerage',action='store_const',const=True,dest='disableRebrokerage',default=False,
                help="disable auto-rebrokerage")
optP.add_option('--useChirpServer',action='store',dest='useChirpServer', default='',
                type='string', help='The CHIRP server where output files are written to. e.g., --useChirpServer voatlas92.cern.ch')
optP.add_option('--useGOForOutput',action='store',dest='useGOForOutput',default='',metavar='GOENDPOINT',
                type='string', help='The Globus Online server where output files are written to. e.g., --useGOForOutput voatlas92.cern.ch')
optP.add_option('--enableJEM',action='store_const',const=True,dest='enableJEM',default=False,
                help="enable JEM")
optP.add_option('--configJEM', action='store', dest='configJEM', default='',
                type='string', help='configration parameters for JEM')
optP.add_option('--cmtConfig', action='store', dest='cmtConfig', default=None,
                type='string', help='CMTCONFIG=i686-slc5-gcc43-opt is used on remote worker-node by default even if you use another CMTCONFIG locally. This option allows you to use another CMTCONFIG remotely. e.g., --cmtConfig x86_64-slc5-gcc43-opt. If you use --libDS together with this option, make sure that the libDS was compiled with the same CMTCONFIG, in order to avoid failures due to inconsistency in binary files')
optP.add_option('--allowTaskDuplication',action='store_const',const=True,dest='allowTaskDuplication',default=False,
                help="As a general rule each task has a unique outDS and history of file usage is recorded per task. This option allows multiple tasks to contribute to the same outDS. Typically useful to submit a new task with the outDS which was used by another broken task. Use this option very carefully at your own risk, since file duplication happens when the second task runs on the same input which the first task successfully processed")
optP.add_option('--skipFilesUsedBy', action='store', dest='skipFilesUsedBy', default='',
                type='string', help='A comma-separated list of TaskIDs. Files used by those tasks are skipped when running a new task')
# athena options
optP.add_option('-c',action='store',dest='singleLine',type='string',default='',metavar='COMMAND',
                help='One-liner, runs before any jobOs')
optP.add_option('-p',action='store',dest='preConfig',type='string',default='',metavar='BOOTSTRAP',
                help='location of bootstrap file')
optP.add_option('-s',action='store_const',const=True,dest='codeTrace',default=False,
                help='show printout of included files')
optP.add_option('--queueData', action='store', dest='queueData', default='',
                type='string', help="Please don't use this option. Only for developers")
optP.add_option('--useNewCode',action='store_const',const=True,dest='useNewCode',default=False,
                help='When task are resubmitted with the same outDS, the original souce code is used to re-run on failed/unprocessed files. This option uploads new source code so that jobs will run with new binaries') 
optP.add_option('--useRucio',action='store_const',const=True,dest='useRucio',default=False,
                help="Use Rucio as DDM backend")
# internal parameters
optP.add_option('--panda_srvURL', action='store', dest='panda_srvURL', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_cacheSrvURL', action='store', dest='panda_cacheSrvURL', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_runConfig', action='store', dest='panda_runConfig', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_srcName', action='store', dest='panda_srcName', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_inDS', action='store', dest='panda_inDS', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_inDSForEP', action='store', dest='panda_inDSForEP', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_origFullExecString', action='store', dest='panda_origFullExecString', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_jobsetID',action='store',dest='panda_jobsetID',default=-1,
                type='int', help='internal parameter for jobsetID')
optP.add_option('--panda_parentJobsetID',action='store',dest='panda_parentJobsetID',default=-1,
                type='int', help='internal parameter for jobsetID')
optP.add_option('--panda_dbRelease', action='store', dest='panda_dbRelease', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_singleLine', action='store', dest='panda_singleLine', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_trf', action='store', dest='panda_trf', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_eventPickRunEvtDat', action='store', dest='panda_eventPickRunEvtDat', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_devidedByGUID',action='store_const',const=True,dest='panda_devidedByGUID',default=False,
                help='internal parameter')
optP.add_option('--panda_suppressMsg',action='store_const',const=True,dest='panda_suppressMsg',default=False,
                help='internal parameter')
optP.add_option('--panda_fullPathJobOs',action='store', dest='panda_fullPathJobOs', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_tagParentFile',action='store', dest='panda_tagParentFile', default='',
                type='string', help='internal parameter')

# parse options
options,args = optP.parse_args()
if options.verbose:
    print options
    print

# display version
from pandatools import PandaToolsPkgInfo
if options.version:
    print "Version: %s" % PandaToolsPkgInfo.release_version
    sys.exit(0)

from pandatools import Client
from pandatools import PsubUtils
from pandatools import AthenaUtils
from pandatools import GlobalConfig
from pandatools import AppConfig
from pandatools import MiscUtils 
from pandatools import PLogger

# update panda-client
if options.update:
    res = PsubUtils.updatePackage(options.verbose)
    if res:
	sys.exit(0)
    else:
	sys.exit(1)

# full execution string
fullExecString = PsubUtils.convSysArgv()

# max size per job
maxTotalSize = Client.maxTotalSize
safetySize   = Client.safetySize

# set grid source file
globalConf = GlobalConfig.getConfig()
if globalConf.grid_src != '' and not os.environ.has_key('PATHENA_GRID_SETUP_SH'):
    os.environ['PATHENA_GRID_SETUP_SH'] = globalConf.grid_src

# get logger
tmpLog = PLogger.getPandaLogger()

# set default
appConf = AppConfig.AppConfig('pathena')
for tmpAppConfKey,tmpAppConfVal in appConf.getConfig().iteritems():
    # check lower characters just in case
    tmpAppConfKeys = [tmpAppConfKey,tmpAppConfKey.lower()]
    for tmpKey in tmpAppConfKeys:
        if hasattr(options,tmpKey):
            tmpSetAttFlag = False
	    if getattr(options,tmpKey) in [-1,0,None,'','AUTO',False]:
		setattr(options,tmpKey,tmpAppConfVal)
		tmpSetAttFlag = True
            elif getattr(options,tmpKey) == []:
                tmpSetValue = tmpAppConfVal.split(',')
                if '' in tmpSetValue:
                    tmpSetValue.remove('')
                setattr(options,tmpKey,tmpSetValue)
                tmpSetAttFlag = True
            if tmpSetAttFlag:
                # append parameter to metadata
                if isinstance(tmpAppConfVal,types.BooleanType):
                    fullExecString += ' --%s' % tmpKey
                else:
                    fullExecString += ' --%s=%s' % (tmpKey,tmpAppConfVal)
                if options.verbose:
                    tmpLog.debug("Use default option in panda.cfg %s=%s" % (tmpKey,getattr(options,tmpKey)))
            break

# use dev server
if options.devSrv:
    Client.useDevServer()

# use INTR server
if options.intrSrv:
    Client.useIntrServer()
    
# set server
if options.panda_srvURL != '':
    Client.setServer(options.panda_srvURL)
if options.panda_cacheSrvURL != '':
    Client.setCacheServer(options.panda_cacheSrvURL)

# version check
PsubUtils.checkPandaClientVer(options.verbose)

# obsolete options
if options.useOldStyleOutput:
    options.useOldStyleOutput = False
    tmpLog.warning("disabled --useOldStyleOutput since it is obsolete")

# use old trf parameterization
if options.useOldTRF:
    options.useNewTRF = False

# noCompile uses noBuild stuff
if options.noCompile:
    if options.nobuild:
        tmpLog.error("--noBuild and --noCompile cannot be used simultaneously")
        sys.exit(EC_Config)
    options.nobuild = True

# syntax sugar
if options.nJobs > 0:
    options.split = options.nJobs

# files to be deleted
delFilesOnExit = []
                            
# suffix for shadow dataset
suffixShadow = Client.suffixShadow

# save current dir
currentDir = os.path.realpath(os.getcwd())

brokerageLogs  = []
userBrokerLogs = []

# exclude sites
if options.excludedSite != []:
    options.excludedSite = PsubUtils.splitCommaConcatenatedItems(options.excludedSite)

# use certain sites
includedSite = None
if re.search(',',options.site) != None:
    includedSite = PsubUtils.splitCommaConcatenatedItems([options.site])
    options.site = 'AUTO'

# site specified
siteSpecified = True
if options.site == 'AUTO':
    siteSpecified = False

# cloud specified
if options.cloud != defaultCloud:
    # add logging info
    userBrokerLogs = PsubUtils.getUserBrokerageInfo(options.cloud,'cloud',userBrokerLogs)

# list of output files which can be skipped
options.allowNoOutput = options.allowNoOutput.split(',')

# use outputPath as outDS
if Client.isDQ2free(options.site):
    if options.outDS != '':
	options.outputPath = options.outDS
    else:
	options.outputPath = './'
    options.outDS = options.outputPath
else:
    # enforce to use output dataset container
    if not options.useOldStyleOutput and not options.outDS.endswith('/'):
        options.outDS = options.outDS + '/'

# keep original outDS
original_outDS_Name = options.outDS
     
# reset crossSite unless container is used for output 
if not original_outDS_Name.endswith('/'):
    options.crossSite = 0
    options.panda_jobsetID = None

usingContainerForOut = original_outDS_Name.endswith('/')

# read datasets from file
if options.inDsTxt != '':
    options.inDS = PsubUtils.readDsFromFile(options.inDsTxt)

# disable expiring file check
if options.useShortLivedReplicas:
    Client.useExpiringFiles()
    
# error
if options.outDS == '':
    tmpLog.error("no outDS is given\n pathena [--inDS input] --outDS output myJobO.py")
    sys.exit(EC_Config)
if options.split < -1 :
    tmpLog.error("Number of jobs should be a positive integer")
    sys.exit(EC_Config)
if options.pfnList != '':
    if options.inDS != '': 
        tmpLog.error("--pfnList and --inDS cannot be used at the same time")
        sys.exit(EC_Config)
    if options.shipinput:
        tmpLog.error("--shipInput and --inDS cannot be used at the same time")
        sys.exit(EC_Config)
    if options.site == 'AUTO':
	tmpLog.error("--site must be specified when --pfnList is used")
	sys.exit(EC_Config)

# libDS
libds_file = '%s/libds_pathena.dat' % os.environ['PANDA_CONFIG_ROOT']
if options.libDS == 'LAST':
    if not os.path.exists(libds_file):
        tmpLog.error("LAST cannot be used until you submit at least one job without --libDS")
        sys.exit(EC_Config)
    # read line
    tmpFile = open(libds_file)
    tmpLibDS = tmpFile.readline()
    tmpFile.close()
    # remove \n
    tmpLibDS = tmpLibDS.replace('\n','')
    # set
    options.libDS = tmpLibDS

# absolute path for PFN list
usePfnList = False
if options.pfnList != '':
    options.pfnList = os.path.realpath(options.pfnList)
    usePfnList = True

# burst submission
if options.burstSubmit != '':
    # don't scan LRC/LFC
    options.skipScan = True
    # reset cloud/site. They will be overwritten at submission
    options.cloud = None
    options.site  = None
    # disable individual output
    options.individualOutDS = False
    # check libDS stuff
    if options.libDS != '' or (options.nobuild and not options.noCompile):
        tmpLog.error("--libDS or --nobuild cannot be used together with --burstSubmit")
        sys.exit(EC_Config)
        

# split options are mutually exclusive
if (options.nFilesPerJob > 0 and options.nEventsPerJob > 0 and options.nGBPerJob != -1):
    tmpLog.error("split by files and split by events and split by file size can not be used simultaneously")
    sys.exit(EC_Config)

# split options are mutually exclusive
if (options.nEventsPerJob > 0 and options.nGBPerJob != -1):
    tmpLog.error("split by events and split by file size can not be used simultaneously")
    sys.exit(EC_Config)

# check nGBPerJob
if not options.nGBPerJob in [-1,'MAX']:
    # convert to int
    try:
        if options.nGBPerJob != 'MAX':
            options.nGBPerJob = int(options.nGBPerJob)
    except:
        tmpLog.error("nGBPerJob must be an integer or MAX")
        sys.exit(EC_Config)
    # check negative    
    if options.nGBPerJob <= 0:
        tmpLog.error("nGBPerJob must be positive")
        sys.exit(EC_Config)
    
# trf parameter
if options.panda_trf != '':
    options.trf = urllib.unquote(options.panda_trf)
if options.trf == False:
    orig_trfStr = ''
else:
    orig_trfStr = options.trf

# one liner
if options.panda_singleLine != '':
    options.singleLine = urllib.unquote(options.panda_singleLine)

# AMI event-level split
if options.useAMIEventLevelSplit == None:
    if options.inDS.startswith('data') or options.goodRunListXML != '':
        # use AMI for real data since the number of events per file is not uniform
        options.useAMIEventLevelSplit = True
    else:
        options.useAMIEventLevelSplit = False

# check DBRelease
if options.dbRelease != '' and (options.dbRelease.find(':') == -1 and options.dbRelease !='LATEST'):
    tmpLog.error("invalid argument for --dbRelease. Must be DatasetName:FileName or LATEST")  
    sys.exit(EC_Config)

# Good Run List
if options.goodRunListXML != '' and options.inDS != '':
    tmpLog.error("cannnot use --goodRunListXML and --inDS at the same time")
    sys.exit(EC_Config)

# event picking
if options.eventPickEvtList != '' and options.inDS != '':
    tmpLog.error("cannnot use --eventPickEvtList and --inDS at the same time")
    sys.exit(EC_Config)

# param check for event picking
if options.eventPickEvtList != '':
    if options.eventPickDataType == '':
        tmpLog.error("--eventPickDataType must be specified")
        sys.exit(EC_Config)
    if options.trf != False:
        tmpLog.error("--eventPickEvtList doesn't work with --trf until official transformations support event picking")
        sys.exit(EC_Config)
        
    
# additinal files
options.extFile = options.extFile.split(',')
try:
    options.extFile.remove('')
except:
    pass
options.extOutFile = re.sub(' ','',options.extOutFile)
options.extOutFile = options.extOutFile.split(',')
try:
    options.extOutFile.remove('')
except:
    pass

# user-specified merging script
if options.mergeScript != '':
    # enable merging
    options.mergeOutput = True
    # add it to extFile
    if not options.mergeScript in options.extFile:
        options.extFile.append(options.mergeScript)

# removed datasets
if options.removedDS == '':
    options.removedDS = []
else:
    options.removedDS = options.removedDS.split(',')
    
# glue packages
options.gluePackages = options.gluePackages.split(',')
try:
    options.gluePackages.remove('')
except:
    pass

# set excludeFile
AthenaUtils.setExcludeFile(options.excludeFile)

# mapping for extra stream names
if options.appendStrToExtStream:
    AthenaUtils.enableExtendedExtStreamName()

# set ara on when ares is used
if options.ares:
    options.ara = True

# output files for ARA
if options.ara and options.araOutFile == '':
    tmpLog.error("--araOutFile is needed when ARA (--ara) is used")
    sys.exit(EC_Config)
for tmpName in options.araOutFile.split(','):
    if tmpName != '':
        options.extOutFile.append(tmpName)

# file list
tmpList = options.filelist.split(',')
options.filelist = []
for tmpItem in tmpList:
    if tmpItem == '':
        continue
    # wild card
    tmpItem = tmpItem.replace('*','.*')
    # append
    options.filelist.append(tmpItem) 
# read file list from file
if options.inputFileList != '':
    rFile = open(options.inputFileList)
    for line in rFile:
        line = re.sub('\n','',line)
        line = line.strip()
        if line != '':
            options.filelist.append(line)
    rFile.close()

# removed files
if options.removeFileList == '':
    # empty
    options.removeFileList = []
else:
    # read from file
    rList = []
    rFile = open(options.removeFileList)
    for line in rFile:
        line = re.sub('\n','',line)        
        rList.append(line)
    rFile.close()
    options.removeFileList = rList

# suppressed streams
options.supStream = options.supStream.upper().split(',')
try:
    options.supStream.remove('')
except:
    pass

# set nFilesPerJob for MC data
if options.mcData != '':
    options.nFilesPerJob = 1


# split related
if options.split > 0:
    # set nFiles when nEventsPerJob and nEventsPerFile are set
    if options.nEventsPerJob > 0 and options.nEventsPerFile > 0:
        if options.nEventsPerJob >= options.nEventsPerFile:
            options.nfiles = options.nEventsPerJob / options.nEventsPerFile * options.split
        else:
            options.nfiles =  options.split / (options.nEventsPerFile / options.nEventsPerJob)
            if options.nfiles == 0:
                options.nfiles = 1

    # set nFiles when nFilesPerJob is set
    if options.nFilesPerJob > 0 and options.nfiles == 0:
        options.nfiles = options.nFilesPerJob * options.split

    # set nFiles per job when nFiles is set
    if options.nFilesPerJob < 0 and options.nfiles > 0:
        options.nFilesPerJob = options.nfiles / options.split
        if options.nFilesPerJob == 0:
            options.nFilesPerJob = 1

# check
if options.inDS != '' and options.split > 0 and options.nFilesPerJob < 0 and options.nfiles == 0 and options.nEventsPerJob < 0:
    tmpLog.error("--split requires --nFilesPerJob or --nFiles or --nEventsPerJob when --inDS is specified")
    sys.exit(EC_Config)

# check grid-proxy
gridPassPhrase,vomsFQAN = PsubUtils.checkGridProxy('',False,options.verbose,options.vomsRoles)

# add allowed sites# 
if (not siteSpecified) and options.burstSubmit == '':
    tmpSt = Client.addAllowedSites(options.verbose)
    if not tmpSt:
        tmpLog.error("Failed to get allowed site list")
        sys.exit(EC_Config)

# correct site
if options.site != 'AUTO' and options.burstSubmit == '':
    origSite = options.site
    # patch for BNL
    if options.site in ['BNL',"ANALY_BNL"]:
        options.site = "ANALY_BNL_SHORT"
    # patch for CERN
    if options.site in ['CERN']:
        options.site = "ANALY_CERN_XROOTD"        
    # try to convert DQ2ID to PandaID
    pID = PsubUtils.convertDQ2toPandaID(options.site)
    if pID != '':
        options.site = pID
    # add ANALY
    if not options.site.startswith('ANALY_'):
        options.site = 'ANALY_%s' % options.site
    # check
    if not Client.PandaSites.has_key(options.site):
        tmpLog.error("unknown siteID:%s" % origSite)
        sys.exit(EC_Config)
    # add logging info
    userBrokerLogs = PsubUtils.getUserBrokerageInfo(options.site,'site',userBrokerLogs)
    # set cloud
    options.cloud = Client.PandaSites[options.site]['cloud']

# check cloud
if options.cloud != None and options.burstSubmit == '':
    foundCloud = False
    for tmpID,spec in Client.PandaSites.iteritems():
        if options.cloud == spec['cloud']:
            foundCloud = True
            break
    if not foundCloud:
        tmpLog.error("unsupported cloud:%s" % options.cloud)
        sys.exit(EC_Config)

# get DN
distinguishedName = PsubUtils.getDN()

# get nickname
nickName = PsubUtils.getNickname()

if nickName == '':
    sys.exit(EC_Config)

# set Rucio accounting
PsubUtils.setRucioAccount(nickName,'pathena',True)

# check outDS format
if not PsubUtils.checkOutDsName(options.outDS,distinguishedName,options.official,nickName,
                                options.site,vomsFQAN,options.mergeOutput):
    tmpLog.error("invalid output datasetname:%s" % options.outDS)
    sys.exit(EC_Config)

# convert in/outTarBall to full path
if options.inTarBall != '':
    options.inTarBall = os.path.abspath(os.path.expanduser(options.inTarBall))
if options.outTarBall != '':
    options.outTarBall = os.path.abspath(os.path.expanduser(options.outTarBall))

# convert n/outRunConfig to full path
if options.inRunConfig != '':
    options.inRunConfig = os.path.abspath(os.path.expanduser(options.inRunConfig))
if options.outRunConfig != '':
    options.outRunConfig = os.path.abspath(os.path.expanduser(options.outRunConfig))

# check maxCpuCount 
if options.maxCpuCount > Client.maxCpuCountLimit:
    tmpLog.error("too large maxCpuCount. Must be less than %s" % Client.maxCpuCountLimit)
    sys.exit(EC_Config)

# create tmp dir
if options.tmpDir == '':
    tmpDir = '%s/%s' % (currentDir,MiscUtils.wrappedUuidGen())
else:
    tmpDir = '%s/%s' % (os.path.abspath(options.tmpDir),MiscUtils.wrappedUuidGen())    
os.makedirs(tmpDir)

# set tmp dir in Client
Client.setGlobalTmpDir(tmpDir)

# exit action
def _onExit(dir,files):
    for tmpFile in files:
        commands.getoutput('rm -rf %s' % tmpFile)        
    commands.getoutput('rm -rf %s' % dir)
atexit.register(_onExit,tmpDir,delFilesOnExit)


# get Athena versions
stA,retA = AthenaUtils.getAthenaVer()
# failed
if not stA:
    sys.exit(EC_CMT)
workArea  = retA['workArea'] 
athenaVer = retA['athenaVer'] 
groupArea = retA['groupArea'] 
cacheVer  = retA['cacheVer'] 
nightVer  = retA['nightVer']

# overwrite with athenaTag
if options.athenaTag != '':
    athenaVer = ''
    cacheVer  = ''
    nightVer  = ''
    # get list of Athena projects
    listProjects = Client.getCachePrefixes(options.verbose)
    items = options.athenaTag.split(',')
    usingNightlies = False
    for item in items:
        # releases
        match = re.search('^(\d+\.\d+\.\d+)',item)
        if match != None:
            athenaVer = match.group(1)
            # cache
	    cmatch = re.search('^(\d+\.\d+\.\d+\.\d+\.*\d*)$',item)
	    if cmatch != None:
                #cacheVer += '_%s' % cmatch.group(1)    #Modified by Yohei                                                  
                cacheVer += ',%s' % cmatch.group(1)
                #print "Yohei, cacheVer: ", cacheVer               
        else:
            # nightlies
            match = re.search('^(\d+\.\d+\.X|\d+\.X\.\d+)$',item)
            if match != None:
                athenaVer = 'Atlas-%s' % match.group(1)
        # project
        if item.startswith('Atlas') or item.startswith('Athena') or item in listProjects:
            # ignore AtlasOffline
            if item in ['AtlasOffline']:
                continue
            cacheVer = '-'+item+cacheVer
        # nightlies    
        if item.startswith('rel_'):
            usingNightlies = True
            if 'dev' in items:
                athenaVer = 'Atlas-dev'
            elif 'devval' in items:
                athenaVer = 'Atlas-devval'
            cacheVer  = '-AtlasOffline_%s' % item
	# CMTCONFIG
        if item in ['32','64']:
            tmpLog.warning("%s in --athenaTag is unsupported. Please use --cmtConfig instead" % item)
        if options.verbose:
            print 'item %s,  %s' %(item, cacheVer)
    # check cache
    if re.search('^-.+_.+$',cacheVer) == None:
        if re.search('^_\d+\.\d+\.\d+\.\d+$',cacheVer) != None:
            # use AtlasProduction
            cacheVer = '-AtlasProduction'+cacheVer
	elif re.search('^-.+,.+$',cacheVer) != None:   #Added by Yohei                                                      
            # cacheVer to be 'AtlasProduction-20.7.3..' or 'AtlasP1HLT-20.7.3..'                                            
            cacheVer = cacheVer[1:]
	    
        elif 'AthAnalysisBase' in cacheVer or 'AthAnalysis' in cacheVer:
            # AthAnalysis
            cacheVer  = cacheVer + '_%s' % athenaVer
            athenaVer = ''
	elif 'Athena' in cacheVer:
            # Athena                                                                     
            cacheVer  = cacheVer[1:] + ',%s' % athenaVer
        else:
            # unknown
            cacheVer = ''
    # use dev nightlies
    if usingNightlies and athenaVer == '':
        athenaVer = 'Atlas-dev'

print "Yohei, cacheVer: ", cacheVer


# set CMTCONFIG
options.cmtConfig = AthenaUtils.getCmtConfig(athenaVer,cacheVer,nightVer,options.cmtConfig,verbose=options.verbose)

# check CMTCONFIG
if not AthenaUtils.checkCmtConfig(retA['cmtConfig'],options.cmtConfig,options.nobuild):
    sys.exit(EC_CMT)

tmpLog.info('using CMTCONFIG=%s' % options.cmtConfig)

# get run directory
# remove special characters                    
sString=re.sub('[\+]','.',workArea)
runDir = re.sub('^%s' % sString, '', currentDir)
if runDir == currentDir and not AthenaUtils.useCMake():
    errMsg  = "You need to run pathena in a directory under %s. " % workArea
    errMsg += "If '%s' is a read-only directory, perhaps you did setup Athena without --testarea or the 'here' tag of asetup." % workArea
    tmpLog.error(errMsg)
    sys.exit(EC_Config)
elif runDir == '':
    runDir = '.'
elif runDir.startswith('/'):
    runDir = runDir[1:]
runDir = runDir+'/'

# check unmerge dataset
PsubUtils.checkUnmergedDataset(options.inDS,options.parentDS)

# event picking
if options.eventPickEvtList != '':
    epLockedBy = 'pathena'
    if not options.nosubmit:
        epStat,epOutput = Client.requestEventPicking(options.eventPickEvtList,
                                                     options.eventPickDataType,
                                                     options.eventPickStreamName,
                                                     options.eventPickDS,
                                                     options.eventPickAmiTag,
                                                     options.filelist,
                                                     '',
                                                     options.outDS,
                                                     epLockedBy,
                                                     fullExecString,
                                                     options.eventPickNumSites,
						     options.eventPickWithGUID,
                                                     options.verbose)
        # set input dataset 
        options.inDS = epOutput
    else:
        options.inDS = 'dummy'
    tmpLog.info('requested Event Picking service to stage input as %s' % options.inDS)
    # make run/event list file for event picking
    eventPickRunEvtDat = '%s/ep_%s.dat' % (currentDir,MiscUtils.wrappedUuidGen())
    evI = open(options.eventPickEvtList)
    evO = open(eventPickRunEvtDat,'w')
    evO.write(evI.read())
    # close
    evI.close()
    evO.close()
    # add to be deleted on exit
    delFilesOnExit.append(eventPickRunEvtDat)

# get job options
jobO = ''
if options.trf:
    # replace : to = for backward compatibility
    for optArg in ['DB','RNDM']:
        options.trf = re.sub('%'+optArg+':','%'+optArg+'=',options.trf)
    # use trf's parameters
    jobO = options.trf
else:
    # get jobOs from command-line
    if options.preConfig != '':
        jobO += '-p %s ' % options.preConfig
    if options.singleLine != '':
        options.singleLine = options.singleLine.replace('"','\'')
        jobO += '-c "%s" ' % options.singleLine
    for arg in args:
        jobO += ' %s' % arg
if jobO == "":
    tmpLog.error("no jobOptions is given\n   pathena [--inDS input] --outDS output myJobO.py")
    sys.exit(EC_Config)

# ARA uses trf I/F
if options.ara:
    if options.ares:
        jobO = "athena.py " + jobO        
    elif jobO.endswith(".C"):
        jobO = "root -l " + jobO
    else:
        jobO = "python " + jobO        
    options.trf = jobO


if options.panda_runConfig == '' and options.inRunConfig == '':
    # extract run configuration    
    tmpLog.info('extracting run configuration')
    # run ConfigExtractor for normal jobO 
    ret,runConfig = AthenaUtils.extractRunConfig(jobO,options.supStream,options.useAIDA,options.shipinput,
                                                 options.trf,verbose=options.verbose,
						 useAMI=options.useAMIAutoConf,inDS=options.inDS,
						 tmpDir=tmpDir)
    # save runconfig
    if options.outRunConfig != '':
        cFile = open(options.outRunConfig,'w')
        pickle.dump(runConfig,cFile)
        cFile.close()
else:
    # use a saved file
    if options.panda_runConfig == '':
        options.panda_runConfig = options.inRunConfig
    # load from file
    ret = True
    tmpRunConfFile = open(options.panda_runConfig)
    runConfig = pickle.load(tmpRunConfFile)
    tmpRunConfFile.close()
if not options.trf:
    # extractor failed
    if not ret:
        sys.exit(EC_Extractor)
    # shipped files
    if runConfig.other.inputFiles:
        for fileName in runConfig.other.inputFiles:
            # append .root for tag files
            if runConfig.other.inColl:
                match = re.search('\.root(\.\d+)*$',fileName)
                if match == None:
                    fileName = '%s.root' % fileName
            # check ship files in the current dir
            if not os.path.exists(fileName):
                tmpLog.error("%s needs exist in the current directory when --shipInput is used" % fileName)
                sys.exit(EC_Extractor)
            # append to extFile
            options.extFile.append(fileName)
            if not runConfig.input.shipFiles:
                runConfig.input['shipFiles'] = []
            if not fileName in runConfig.input['shipFiles']:    
                runConfig.input['shipFiles'].append(fileName)
    # generator files
    if runConfig.other.rndmGenFile:
        # append to extFile
        for fileName in runConfig.other.rndmGenFile:
            options.extFile.append(fileName)
    # Condition file
    if runConfig.other.condInput:
        # append to extFile
        for fileName in runConfig.other.condInput:
            if options.addPoolFC == "":
                options.addPoolFC = fileName
            else:
                options.addPoolFC += ",%s" % fileName
    # set default ref name
    if not runConfig.input.collRefName:
        runConfig.input.collRefName = 'Token'
    # check dupication in extOutFile
    if runConfig.output.alloutputs != False:
        if options.verbose:
            tmpLog.debug("output files : %s" % str(runConfig.output.alloutputs)) 
        for tmpExtOutFile in tuple(options.extOutFile):
            if tmpExtOutFile in runConfig.output.alloutputs:
                if not options.panda_suppressMsg:
                    tmpLog.warning("removed %s from extOutFile since it is automatically extracted from Athena. You don't need to specify it in extOutFile"
                                   % tmpExtOutFile)
                options.extOutFile.remove(tmpExtOutFile)
else:
    # parse parameters for trf
    # AMI tag
    newJobO = ''
    for tmpString in jobO.split(';'):
        match = re.search(' AMI=',tmpString)
        if match == None:
            # use original command
            newJobO += (tmpString + ';')
        else:
            tmpLog.info('getting configration from AMI')
            # get configration using GetCommand.py
            com = 'GetCommand.py ' + re.sub('^[^ ]+ ','',tmpString.strip())
            if options.verbose:
                tmpLog.debug(com)
            amiSt,amiOut = commands.getstatusoutput(com)
            amiSt %= 255
            if amiSt != 0:
                tmpLog.error(amiOut)
                errSt =  'Failed to get configuration from AMI. '
                errSt += 'Using AMI=tag in --trf is disallowed since it may overload the AMI server. '
                errSt += 'Please use explicit configuration parameters in --trf'
                tmpLog.error(errSt)
                sys.exit(EC_Config)
            # get full command string
            fullCommand = ''
            for amiStr in amiOut.split('\n'):
                if amiStr != '' and not amiStr.startswith('#') and not amiStr.startswith('*'):
                    fullCommand = amiStr
            # failed to extract configration        
            if fullCommand == '':
                tmpLog.error(amiOut)
                errSt =  "Failed to extract configuration from AMI's output"
                tmpLog.error(errSt)
                sys.exit(EC_Config)
            # replace
            newJobO += (fullCommand + ';')
    # remove redundant ;
    newJobO = newJobO[:-1]
    # replace
    if newJobO != '':
        jobO = newJobO
        if options.verbose:
            tmpLog.debug('new jobO : '+jobO)
    # output                
    oneOut = False
    # replace ; for job sequence
    tmpString = re.sub(';',' ',jobO)
    # look for --outputDAODFile and --reductionConf 
    match = re.search('--outputDAODFile[ =\"\']+([^ \"\',]+)',tmpString)
    outputDAODFile = None
    if match != None:
        outputDAODFile = match.group(1)
        # remove %OUT
        outputDAODFile = re.sub('%OUT\.','',outputDAODFile)
        match = re.search('--reductionConf[ =\"\']+([^ \"\']+)',tmpString)
        if match != None:
            # remove %OUT from outputDAODFile
            jobO = jobO.replace('%OUT.'+outputDAODFile,outputDAODFile)
            # loop over all configs
            reductionConf = match.group(1)
            for reductionItem in reductionConf.split(','):
                reductionItem = reductionItem.strip()
                if reductionItem == '':
                    continue
                # make actual output names for derivation
                tmpOutName = 'DAOD_{0}.{1}'.format(reductionItem,outputDAODFile)
                if not tmpOutName in options.extOutFile:
                    options.extOutFile.append(tmpOutName)
                    oneOut = True
    # look for %OUT
    for tmpItem in tmpString.split():
        match = re.search('\%OUT\.([^ \"\',]+)',tmpItem)
        if match:
            # append basenames to extOutFile
            tmpOutName = match.group(1)
            # skip basename of derivation
            if outputDAODFile != None and outputDAODFile == tmpOutName:
                continue
            if not tmpOutName in options.extOutFile:
                options.extOutFile.append(tmpOutName)
                oneOut = True
    # warning if no output
    if not oneOut:
        if not options.ara:
            tmpLog.warning("argument of --trf doesn't contain any %OUT")

# no output jobs
tmpOutKeys = runConfig.output.keys()
for tmpIgnorKey in ['outUserData','alloutputs']:
    try:
        tmpOutKeys.remove(tmpIgnorKey)
    except:
        pass
if tmpOutKeys == [] and options.extOutFile == [] and not options.noOutput:
    errStr  = "No output stream was extracted from jobOs or --trf. "
    if not options.trf:
	errStr += "If your job defines an output without Athena framework "
	errStr += "(e.g., using ROOT.TFile.Open instead of THistSvc) "
	errStr += "please specify the output filename by using --extOutFile. "
	errStr += "Or if you define the output with a relatively new mechanism "
	errStr += "please report it to Savannah to update the automatic extractor. " 
    errStr += "If you are sure that your job doesn't produce any output file "
    errStr += "(e.g., HelloWorldOptions.py) please use --noOutput. " 
    tmpLog.error(errStr)  
    sys.exit(EC_Extractor)

# set extOutFile to runConfig
if options.extOutFile != []:
    runConfig.output['extOutFile'] = options.extOutFile

# check ship files in the current dir
if not runConfig.input.shipFiles:
    runConfig.input.shipFiles = []
for file in runConfig.input.shipFiles:
    if not os.path.exists(file):
        tmpLog.error("%s needs exist in the current directory when using --shipInput" % file)
        sys.exit(EC_Extractor)

# get random number
runConfig.other['rndmNumbers'] = []
if not runConfig.other.rndmStream:
    runConfig.other.rndmStream = []
if len(runConfig.other.rndmStream) != 0:
    if options.norandom:
        print
        print "Initial random seeds need to be defined."
        print "Enter two numbers for each random stream."
        print "  e.g., PYTHIA : 4789899 989240512"
        print
    for stream in runConfig.other.rndmStream:
        if options.norandom:
            # enter manually
            while True:
                randStr = raw_input("%s : " % stream)
                num = randStr.split()
                if len(num) == 2:
                    break
                print " Two numbers are needed"
            runConfig.other.rndmNumbers.append([int(num[0]),int(num[1])])
        else:
            # automatic
            runConfig.other.rndmNumbers.append([random.randint(1,5000000),random.randint(1,5000000)])
    if options.norandom:
        print
if runConfig.other.G4RandomSeeds == True:
    if options.norandom:
        print
        print "Initial G4 random seeds need to be defined."
        print "Enter one positive number."
        print
        # enter manually
        while True:
            num = raw_input("SimFlags.SeedsG4=")
            try:
                num = int(num)
                if num > 0:
                    runConfig.other.G4RandomSeeds = num
                    break
            except:
                pass
        print    
    else:
        # automatic
        runConfig.other.G4RandomSeeds = random.randint(1,10000)
else:
    # set -1 to disable G4 Random Seeds
    runConfig.other.G4RandomSeeds = -1

# RefName and Query for TAG
if options.trf and AthenaUtils.checkUseTagInTrf(jobO,options.useTagInTRF):
    if options.tagStreamRef == '':
        tmpLog.error("--tagStreamRef is required when you use TAG in --trf")
        sys.exit(EC_Extractor)
    runConfig.input.collRefName = options.tagStreamRef
    if not options.tagStreamRef.endswith('_ref'):
        runConfig.input.collRefName += '_ref'
    # query
    if options.tagQuery != '':
        runConfig.input.tagQuery = options.tagQuery



#####################################################################
# input datasets

if options.inDS != '' or options.shipinput or options.pfnList != '':
    # minimum bias dataset
    if options.trf and jobO.find('%MININ') != -1:
        runConfig.input.inMinBias = True
    if runConfig.input.inMinBias:
        options.minDS,options.nMin = MiscUtils.getDatasetNameAndNumFiles(options.minDS,
                                                                         options.nMin,
                                                                         'Minimum-Bias')
    # low pT minimum bias dataset
    if options.trf and jobO.find('%LOMBIN') != -1:
        runConfig.input.inLoMinBias = True
    if runConfig.input.inLoMinBias:
        options.lowMinDS,options.nLowMin = MiscUtils.getDatasetNameAndNumFiles(options.lowMinDS,
                                                                               options.nLowMin,
                                                                               'Low pT Minimum-Bias')
    # high pT minimum bias dataset
    if options.trf and jobO.find('%HIMBIN') != -1:
        runConfig.input.inHiMinBias = True
    if runConfig.input.inHiMinBias:
        options.highMinDS,options.nHighMin = MiscUtils.getDatasetNameAndNumFiles(options.highMinDS,
                                                                                 options.nHighMin,
                                                                                 'High pT Minimum-Bias')
    # cavern dataset
    if options.trf and jobO.find('%CAVIN') != -1:
        runConfig.input.inCavern = True
    if runConfig.input.inCavern:
        options.cavDS,options.nCav = MiscUtils.getDatasetNameAndNumFiles(options.cavDS,
                                                                         options.nCav,
                                                                         'Cavern')
    # beam halo dataset
    if options.trf and jobO.find('%BHIN') != -1:
        runConfig.input.inBeamHalo = True 
    if runConfig.input.inBeamHalo:
	# use common DS
	if options.useCommonHalo:
            options.beamHaloDS,options.nBeamHalo = MiscUtils.getDatasetNameAndNumFiles(options.beamHaloDS,
                                                                                       options.nBeamHalo,
                                                                                       'BeamHalo')
	else:	
            # get DS for A-side        
            options.beamHaloADS,options.nBeamHaloA = MiscUtils.getDatasetNameAndNumFiles(options.beamHaloADS,
                                                                                         options.nBeamHaloA,
                                                                                         'BeamHalo A-side')
            # get DS for C-side
            options.beamHaloCDS,options.nBeamHaloC = MiscUtils.getDatasetNameAndNumFiles(options.beamHaloCDS,
                                                                                         options.nBeamHaloC,
                                                                                         'BeamHalo C-side')
    # beam gas dataset
    if options.trf and jobO.find('%BGIN') != -1:
        runConfig.input.inBeamGas = True  
    if runConfig.input.inBeamGas: 
	# use common DS
	if options.useCommonGas:
            options.beamGasDS,options.nBeamGas = MiscUtils.getDatasetNameAndNumFiles(options.beamGasDS,
                                                                                     options.nBeamGas,
                                                                                     'BeamGas')
        else:
            # get DS for H
            options.beamGasHDS,options.nBeamGasH = MiscUtils.getDatasetNameAndNumFiles(options.beamGasHDS,
                                                                                       options.nBeamGasH,
                                                                                       'BeamGas Hydrogen')
            # get DS for C
            options.beamGasCDS,options.nBeamGasC = MiscUtils.getDatasetNameAndNumFiles(options.beamGasCDS,
                                                                                       options.nBeamGasC,
                                                                                       'BeamGas Carbon')
            # get DS for O
            options.beamGasODS,options.nBeamGasO = MiscUtils.getDatasetNameAndNumFiles(options.beamGasODS,
                                                                                       options.nBeamGasO,
                                                                                       'BeamGas Oxygen')



#####################################################################
# archive sources and send it to HTTP-reachable location

if options.panda_srcName != '':
    # reuse src
    if options.verbose:
        tmpLog.debug('reuse source files')
    archiveName = options.panda_srcName
    # go to tmp dir
    os.chdir(tmpDir)
    # set jobOs with fullpath
    if options.panda_fullPathJobOs != '':
        AthenaUtils.fullPathJobOs = AthenaUtils.convStrToFullPathJobOs(options.panda_fullPathJobOs)
else:
    if options.inTarBall == '':
        # extract jobOs with full pathnames
        for tmpItem in jobO.split():
            if re.search('^/.*\.py$',tmpItem) != None:
                # set random name to avoid overwriting
                tmpName = tmpItem.split('/')[-1]
                tmpName = '%s_%s' % (MiscUtils.wrappedUuidGen(),tmpName)
                # set
                AthenaUtils.fullPathJobOs[tmpItem] = tmpName

        # copy some athena specific files
        AthenaUtils.copyAthenaStuff(currentDir)

        # set extFile
        AthenaUtils.setExtFile(options.extFile)

        archiveName = ""
        if options.libDS == '' and not (options.nobuild and not options.noCompile):
                # archive with cpack
            if AthenaUtils.useCMake():
                archiveName,archiveFullName = AthenaUtils.archiveWithCpack(True,tmpDir,options.verbose)
            # archive sources
            archiveName,archiveFullName = AthenaUtils.archiveSourceFiles(workArea,runDir,currentDir,tmpDir,
                                                                         options.verbose,options.gluePackages,
                                                                         archiveName=archiveName) 
        else:
                # archive with cpack
            if AthenaUtils.useCMake():
                archiveName,archiveFullName = AthenaUtils.archiveWithCpack(False,tmpDir,options.verbose)
            # archive jobO
            archiveName,archiveFullName = AthenaUtils.archiveJobOFiles(workArea,runDir,currentDir,
                                                                       tmpDir,options.verbose,
                                                                       archiveName=archiveName)
            
        # archive InstallArea
        if options.libDS == '':
            AthenaUtils.archiveInstallArea(workArea,groupArea,archiveName,archiveFullName,
                                           tmpDir,options.nobuild,options.verbose)
        # back to tmp dir        
        os.chdir(tmpDir)
        # remove some athena specific files
        AthenaUtils.deleteAthenaStuff(currentDir)
        # compress
        status,out = commands.getstatusoutput('gzip -f %s' % archiveName)
        if status != 0 or options.verbose:
            print out
        archiveName += '.gz'
        # check archive
        status,out = commands.getstatusoutput('ls -l %s' % archiveName)
        if status != 0:
            print out
            tmpLog.error("Failed to archive working area.\n        If you see 'Disk quota exceeded', try '--tmpDir /tmp'") 
            sys.exit(EC_Archive)

        # check symlinks
        tmpLog.info("checking symbolic links")
        status,out = commands.getstatusoutput('tar tvfz %s' % archiveName)
        if status != 0:
            tmpLog.error("Failed to expand archive")
            sys.exit(EC_Archive)
        symlinks = []    
        for line in out.split('\n'):
            items = line.split()
            if items[0].startswith('l') and items[-1].startswith('/'):
                symlinks.append(line)
        if symlinks != []:
            tmpStr  = "Found some unresolved symlinks which may cause a problem\n"
            tmpStr += "     See, e.g., http://savannah.cern.ch/bugs/?43885\n"
            tmpStr += "   Please ignore if you believe they are harmless"
            tmpLog.warning(tmpStr)
            for symlink in symlinks:
                print "  %s" % symlink
    else:
        # go to tmp dir
        os.chdir(tmpDir)
        # use a saved copy
        if options.libDS == '' and not (options.nobuild and not options.noCompile):
            archiveName     = 'sources.%s.tar' % MiscUtils.wrappedUuidGen()
            archiveFullName = "%s/%s" % (tmpDir,archiveName)
        else:
            archiveName     = 'jobO.%s.tar' % MiscUtils.wrappedUuidGen()
            archiveFullName = "%s/%s" % (tmpDir,archiveName)
        # make copy to avoid name duplication
        shutil.copy(options.inTarBall,archiveFullName)
        
    # save
    if options.outTarBall != '':
        shutil.copy(archiveName,options.outTarBall)

    # put sources/jobO via HTTP POST
    if not options.nosubmit:
        tmpLog.info("uploading source/jobO files")
        status,out = Client.putFile(archiveName,options.verbose,useCacheSrv=True,reuseSandbox=True)
	if out.startswith('NewFileName:'):
	    # found the same input sandbox to reuse 
	    archiveName = out.split(':')[-1]
        elif out != 'True':
            # failed
            print out
            tmpLog.error("Failed with %s" % status)
            sys.exit(EC_Post)
        # good run list
        if options.goodRunListXML != '':
            options.goodRunListXML = PsubUtils.uploadGzippedFile(options.goodRunListXML,currentDir,tmpLog,delFilesOnExit,
                                                                 options.nosubmit,options.verbose)




# special handling
specialHandling = ''
if options.express:
    specialHandling += 'express,'
if options.debugMode:
    specialHandling += 'debug,'
specialHandling = specialHandling[:-1]


if options.verbose:
    print "== parameters =="
    print "Site       : %s" % options.site
    print "Athena     : %s" % athenaVer
    if groupArea != '':
        print "Group Area : %s" % groupArea
    if cacheVer != '':
        print "ProdCache  : %s" % cacheVer[1:]
    if nightVer != '':
        print "Nightly    : %s" % nightVer[1:]        
    print "cmtConfig  : %s" % AthenaUtils.getCmtConfig(athenaVer,cacheVer,nightVer,options.cmtConfig)	
    print "RunDir     : %s" % runDir
    print "jobO       : %s" % jobO.lstrip()


####################################################################3
# submit jobs

# read jobID
jobDefinitionID = 1
jobid_file = '%s/pjobid.dat' % os.environ['PANDA_CONFIG_ROOT']
if os.path.exists(jobid_file):
    try:
        # read line
        tmpJobIdFile = open(jobid_file)
        tmpID = tmpJobIdFile.readline()
        tmpJobIdFile.close()
        # remove \n
        tmpID = tmpID.replace('\n','')
        # convert to int
        jobDefinitionID = long(tmpID) + 1
    except:
        pass

# look for pandatools package
for path in sys.path:
    if path == '':
        path = curDir
    if os.path.exists(path) and os.path.isdir(path) and 'pandatools' in os.listdir(path):
        # make symlink for module name.
        os.symlink('%s/pandatools' % path,'taskbuffer')
        break

# append tmpdir to import taskbuffer module
sys.path = [tmpDir]+sys.path
from taskbuffer.JobSpec  import JobSpec
from taskbuffer.FileSpec import FileSpec

# job name
jobName = MiscUtils.wrappedUuidGen()

# make task
taskParamMap = {}
taskParamMap['taskName'] = options.outDS
if not options.allowTaskDuplication:
    taskParamMap['uniqueTaskName'] = True
taskParamMap['vo'] = 'atlas'
taskParamMap['architecture'] = AthenaUtils.getCmtConfig(athenaVer,cacheVer,nightVer,options.cmtConfig)
if athenaVer != '':
    taskParamMap['transUses'] = 'Atlas-%s' % athenaVer
else:
    taskParamMap['transUses'] = athenaVer
taskParamMap['transHome'] = 'AnalysisTransforms'+cacheVer+nightVer
taskParamMap['transHome'] = cacheVer+nightVer
taskParamMap['processingType'] = 'panda-client-{0}-jedi-athena'.format(PandaToolsPkgInfo.release_version)
if options.trf:
    taskParamMap['processingType'] += '-trf'
if options.eventPickEvtList != '':
    taskParamMap['processingType'] += '-evp'
    taskParamMap['waitInput'] = 1
if options.goodRunListXML != '':
    taskParamMap['processingType'] += '-grl'
if options.prodSourceLabel == '':
    taskParamMap['prodSourceLabel'] = 'user'
else:
    taskParamMap['prodSourceLabel'] = options.prodSourceLabel
if options.site != 'AUTO':
    taskParamMap['site'] = options.site
else:
    taskParamMap['site'] = None
taskParamMap['cloud'] = options.cloud
taskParamMap['excludedSite'] = options.excludedSite
if includedSite != None and includedSite != []:
    taskParamMap['includedSite'] = includedSite
else:
    taskParamMap['includedSite'] = None
if options.nfiles > 0:
    taskParamMap['nFiles'] = options.nfiles
if options.nFilesPerJob > 0:
    taskParamMap['nFilesPerJob'] = options.nFilesPerJob
if not options.nGBPerJob in [-1,'MAX']:
    # don't set MAX since it is the defalt on the server side
    taskParamMap['nGBPerJob'] = options.nGBPerJob
if options.nEventsPerJob > 0:
    taskParamMap['nEventsPerJob'] = options.nEventsPerJob
    if options.nEventsPerFile <= 0:
        taskParamMap['useRealNumEvents'] = True
if options.nEventsPerFile > 0:
    taskParamMap['nEventsPerFile'] = options.nEventsPerFile
if options.split > 0 and options.nEventsPerJob > 0:
    taskParamMap['nEvents'] = options.split*options.nEventsPerJob
taskParamMap['cliParams'] = fullExecString
if options.noEmail:
    taskParamMap['noEmail'] = True
if options.skipScout:
    taskParamMap['skipScout'] = True
if options.respectSplitRule:
    taskParamMap['respectSplitRule'] = True
if options.disableAutoRetry:
    taskParamMap['disableAutoRetry'] = 1
if options.workingGroup != None:
    taskParamMap['workingGroup'] = options.workingGroup
if options.official:
    taskParamMap['official'] = True
if options.useNewCode:
    taskParamMap['fixedSandbox'] = archiveName
if options.useRucio:
    taskParamMap['ddmBackEnd'] = 'rucio'
if options.maxCpuCount > 0:
    taskParamMap['walltime'] = -options.maxCpuCount
if options.memory > 0:
    taskParamMap['ramCount'] = options.memory
if options.skipFilesUsedBy != '':
    taskParamMap['skipFilesUsedBy'] = options.skipFilesUsedBy
# source URL
matchURL = re.search("(http.*://[^/]+)/",Client.baseURLCSRVSSL)
if matchURL != None:
    taskParamMap['sourceURL'] = matchURL.group(1)
# middle name
if options.addNthFieldOfInFileToLFN != '':
    taskParamMap['addNthFieldToLFN'] = options.addNthFieldOfInFileToLFN
    taskParamMap['useFileAsSourceLFN'] = True
elif options.addNthFieldOfInDSToLFN != '':
    taskParamMap['addNthFieldToLFN'] = options.addNthFieldOfInDSToLFN
# dataset name
logDatasetName = re.sub('/$','.log/',options.outDS)
# log
taskParamMap['log'] = {'dataset': logDatasetName,
                       'container': logDatasetName,
                       'type':'template',
                       'param_type':'log',
                       'value':'{0}.$JEDITASKID.${{SN}}.log.tgz'.format(logDatasetName[:-1])
                       }
if options.addNthFieldOfInFileToLFN != '':
    loglfn  = '{0}.{1}'.format(*logDatasetName.split('.')[:2])
    loglfn += '${MIDDLENAME}.$JEDITASKID._${SN}.log.tgz'
    taskParamMap['log']['value'] = loglfn
if options.spaceToken != '':
    taskParamMap['log']['token'] = options.spaceToken

# make job parameters
taskParamMap['jobParameters'] = []

# build
if options.nobuild and not options.noCompile:
    taskParamMap['jobParameters'] += [
        {'type':'constant',
         'value': '-a {0}'.format(archiveName),
         },
        ]
else:
    taskParamMap['jobParameters'] += [
        {'type':'constant',
         'value': '-l ${LIB}',
         },
        ]
# pre execution string
pStr1 = ''    
if runConfig.other.rndmStream != []:
    pStr1 = "AtRndmGenSvc=Service('AtRndmGenSvc');AtRndmGenSvc.Seeds=["
    for stream in runConfig.other.rndmStream:
        num = runConfig.other.rndmNumbers[runConfig.other.rndmStream.index(stream)]
        pStr1 += "'%s ${RNDMSEED} %s'," % (stream,num[1])
    pStr1 += "]"
    dictItem = {'type':'template',
                'param_type':'number',
                'value':'${RNDMSEED}',
                'hidden':True,
                'offset':runConfig.other.rndmStream[0][0],
                }
    taskParamMap['jobParameters'] += [dictItem]
# split by event option was invoked
pStr2 = ''
if options.nEventsPerJob > 0 and (not options.trf):
    # @ Number of events to be processed per job
    param1 = "theApp.EvtMax=${MAXEVENTS}"
    # @ possibly skip events in a file
    if runConfig.input.noInput:
        pStr2 = param1
    else:
        param2 = "EventSelector.SkipEvents=${SKIPEVENTS}"
        # @ Form a string to add to job parameters
        pStr2 = '%s;%s' % (param1,param2)
# set pre execution parameter
if pStr1 != '' or pStr2 != '':
    if pStr1 == '' or pStr2 == '':
        preStr = pStr1+pStr2
    else:
        preStr = "%s;%s" % (pStr1,pStr2)
    taskParamMap['jobParameters'] += [
        {'type':'constant',
         'value': '-f "',
         'padding':False,
         },
        ]
    taskParamMap['jobParameters'] += PsubUtils.convertParamStrToJediParam(preStr,{},'',
                                                                          False,False)
    taskParamMap['jobParameters'] += [
        {'type':'constant',
         'value': '"',
         },
        ]

# misc
param  = '--sourceURL ${SURL} '
param += '-r {0} '.format(runDir)
# addPoolFC
if options.addPoolFC != "":
    param += '--addPoolFC %s ' % options.addPoolFC
# use corruption checker
if options.corCheck:
    param += '--corCheck '
# disable to skip missing files
if options.notSkipMissing:
    param += '--notSkipMissing '
# given PFN 
if options.pfnList != '':
    param += '--givenPFN '
# create symlink for MC data
if options.mcData != '':
    param += '--mcData %s ' % options.mcData
# run TRF
if options.trf:
    param += '--trf '
# general input format
if options.generalInput:
    param += '--generalInput '
# use local access for TRF and BS
if (options.trf or runConfig.input.inBS) and not options.forceDirectIO:
    param += '--useLocalIO '        
# use theApp.nextEvent
if options.useNextEvent:
    param += '--useNextEvent '
# use CMake
if AthenaUtils.useCMake():
    param += "--useCMake "
# use code tracer 
if options.codeTrace:
    param += '--codeTrace '
# debug parameters
if options.queueData != '':
    param += "--overwriteQueuedata=%s " % options.queueData
# read TAG
if runConfig.input.inColl or (options.trf and AthenaUtils.checkUseTagInTrf(jobO,options.useTagInTRF)):
    param += '-c '
# read BS    
if runConfig.input.inBS:
    param += '-b '
# use back navigation
if runConfig.input.backNavi:
    param += '-e '
# ship input
if options.shipinput:
    param += '--shipInput '
# event picking
if options.eventPickEvtList != '':
    param += '--eventPickTxt=%s ' % eventPickRunEvtDat.split('/')[-1]
# assign
if param != '':
    taskParamMap['jobParameters'] += [
        {'type':'constant',
         'value': param,
         },
        ]

# input
inputMap = {}
if options.inDS != '':
    tmpDict = {'type':'template',
               'param_type':'input',
               'value':'-i "${IN/T}"',
               'dataset':options.inDS,
               'expand':True,
               'exclude':'\.log\.tgz(\.\d+)*$',
               }
    if options.inputType != '':
        tmpDict['include'] = options.inputType
    if options.filelist != []:
        tmpDict['files'] = options.filelist
    taskParamMap['jobParameters'].append(tmpDict)
    taskParamMap['dsForIN'] = options.inDS
    inputMap['IN'] = options.inDS
elif options.pfnList != '':
    taskParamMap['pfnList'] = PsubUtils.getListPFN(options.pfnList)
    # use noInput mecahism
    taskParamMap['noInput'] = True
    if options.nfiles == 0:
        taskParamMap['nFiles'] = len(taskParamMap['pfnList'])
    taskParamMap['jobParameters'] += [
        {'type':'constant',
         'value':'-i "${IN/T}"',
         },
        ]
elif options.goodRunListXML != '':
    tmpDict = {'type':'template',
               'param_type':'input',
               'value':'-i "${IN/T}"',
               'dataset':'%%INDS%%',
	       'expand':True,
               'exclude':'\.log\.tgz(\.\d+)*$',
               'files':'%%INLFNLIST%%',
               }
    taskParamMap['jobParameters'].append(tmpDict)
    taskParamMap['dsForIN'] = '%%INDS%%'
else:
    # no input
    taskParamMap['noInput'] = True
    if options.nEventsPerJob > 0:
        taskParamMap['nEventsPerJob'] = options.nEventsPerJob
    else:
        taskParamMap['nEventsPerJob'] = 1
    if options.split > 0:
        taskParamMap['nEvents'] = options.split
    else:
        taskParamMap['nEvents'] = 1
    taskParamMap['nEvents'] *= taskParamMap['nEventsPerJob']
    taskParamMap['jobParameters'] += [
        {'type':'constant',
         'value': '-i "[]"',
         },
        ]

# extract DBR for --trf
dbrInTRF = False
if options.trf:
    tmpMatch = re.search('%DB=([^ \'\";]+)',jobO)
    if tmpMatch != None:
        options.dbRelease = tmpMatch.group(1)
        dbrInTRF = True
# param for DBR     
if options.dbRelease != '':
    dbrDS = options.dbRelease.split(':')[0]
    # change LATEST to DBR_LATEST
    if dbrDS == 'LATEST':
        dbrDS = 'DBR_LATEST'
    dictItem = {'type':'template',
                'param_type':'input',
                'value':'--dbrFile=${DBR}',
                'dataset':dbrDS,
                }
    taskParamMap['jobParameters'] += [dictItem]
    # no expansion
    if dbrInTRF:
        dictItem = {'type':'constant',
                    'value':'--noExpandDBR',
                    }
        taskParamMap['jobParameters'] += [dictItem]

# minimum bias
minBiasStream = ''        
if options.minDS != '':
    dictItem = MiscUtils.makeJediJobParam('${MININ}',options.minDS,'input',hidden=True,
                                          expand=True,exclude='\.log\.tgz(\.\d+)*$',
                                          nFilesPerJob=options.nMin,useNumFilesAsRatio=True,
                                          randomAtt=options.randomMin,reusableAtt=True)
    taskParamMap['jobParameters'] += dictItem
    inputMap['MININ'] = options.minDS
    minBiasStream += 'MININ,'
if options.lowMinDS != '':
    dictItem = MiscUtils.makeJediJobParam('${LOMBIN}',options.lowMinDS,'input',hidden=True,
                                          expand=True,exclude='\.log\.tgz(\.\d+)*$',
                                          nFilesPerJob=options.nLowMin,useNumFilesAsRatio=True,
                                          randomAtt=options.randomMin,reusableAtt=True)
    taskParamMap['jobParameters'] += dictItem
    inputMap['LOMBIN'] = options.lowMinDS
    minBiasStream += 'LOMBIN,'
if options.highMinDS != '':
    dictItem = MiscUtils.makeJediJobParam('${HIMBIN}',options.highMinDS,'input',hidden=True,
                                          expand=True,exclude='\.log\.tgz(\.\d+)*$',
                                          nFilesPerJob=options.nHighMin,useNumFilesAsRatio=True,
                                          randomAtt=options.randomMin,reusableAtt=True)
    taskParamMap['jobParameters'] += dictItem
    inputMap['HIMBIN'] = options.highMinDS
    minBiasStream += 'HIMBIN,'
minBiasStream = minBiasStream[:-1]
if minBiasStream != '':
    dictItem = {'type':'constant',
                'value':'-m "${{{0}/T}}"'.format(minBiasStream),
                }
    taskParamMap['jobParameters'] += [dictItem]
    
    
# cavern
if options.cavDS != '':
    dictItem = MiscUtils.makeJediJobParam('-n "${CAVIN/T}"',options.cavDS,'input',
                                          expand=True,exclude='\.log\.tgz(\.\d+)*$',
                                          nFilesPerJob=options.nCav,useNumFilesAsRatio=True,
                                          randomAtt=options.randomCav,reusableAtt=True)
    taskParamMap['jobParameters'] += dictItem
    inputMap['CAVIN'] = options.cavDS


# beam halo
beamHaloStream = ''
if options.beamHaloDS != '':
    dictItem = MiscUtils.makeJediJobParam('${BHIN}',options.beamHaloDS,'input',hidden=True,
                                          expand=True,exclude='\.log\.tgz(\.\d+)*$',
                                          nFilesPerJob=options.nBeamHalo,useNumFilesAsRatio=True,
                                          reusableAtt=True)
    taskParamMap['jobParameters'] += dictItem
    inputMap['BHIN'] = options.beamHaloDS
    beamHaloStream += 'BHIN,'    
if options.beamHaloADS != '':
    dictItem = MiscUtils.makeJediJobParam('${BHAIN}',options.beamHaloADS,'input',hidden=True,
                                          expand=True,exclude='\.log\.tgz(\.\d+)*$',
                                          nFilesPerJob=options.nBeamHaloA,useNumFilesAsRatio=True,
                                          reusableAtt=True)
    taskParamMap['jobParameters'] += dictItem
    inputMap['BHAIN'] = options.beamHaloADS
    beamHaloStream += 'BHAIN,'
if options.beamHaloCDS != '':
    dictItem = MiscUtils.makeJediJobParam('${BHCIN}',options.beamHaloCDS,'input',hidden=True,
                                          expand=True,exclude='\.log\.tgz(\.\d+)*$',
                                          nFilesPerJob=options.nBeamHaloC,useNumFilesAsRatio=True,
                                          reusableAtt=True)
    taskParamMap['jobParameters'] += dictItem
    inputMap['BHCIN'] = options.beamHaloCDS
    beamHaloStream += 'BHCIN,'    
beamHaloStream = beamHaloStream[:-1]
if beamHaloStream != '':
    dictItem = {'type':'constant',
                'value':'--beamHalo "${{{0}/T}}"'.format(beamHaloStream)
                }
    taskParamMap['jobParameters'] += [dictItem]


# beam gas
beamGasStream = ''
if options.beamGasDS != '':
    dictItem = MiscUtils.makeJediJobParam('${BGIN}',options.beamGasDS,'input',hidden=True,
                                          expand=True,exclude='\.log\.tgz(\.\d+)*$',
                                          nFilesPerJob=options.nBeamGas,useNumFilesAsRatio=True,
                                          reusableAtt=True)
    taskParamMap['jobParameters'] += dictItem
    inputMap['BGIN'] = options.beamGasDS
    beamGasStream += 'BGIN,'    
if options.beamGasHDS != '':
    dictItem = MiscUtils.makeJediJobParam('${BGHIN}',options.beamGasHDS,'input',hidden=True,
                                          expand=True,exclude='\.log\.tgz(\.\d+)*$',
                                          nFilesPerJob=options.nBeamGasH,useNumFilesAsRatio=True,
                                          reusableAtt=True)
    taskParamMap['jobParameters'] += dictItem
    inputMap['BGHIN'] = options.beamGasHDS
    beamGasStream += 'BGHIN,'    
if options.beamGasCDS != '':
    dictItem = MiscUtils.makeJediJobParam('${BGCIN}',options.beamGasCDS,'input',hidden=True,
                                          expand=True,exclude='\.log\.tgz(\.\d+)*$',
                                          nFilesPerJob=options.nBeamGasC,useNumFilesAsRatio=True,
                                          reusableAtt=True)
    taskParamMap['jobParameters'] += dictItem
    inputMap['BGCIN'] = options.beamGasHDS
    beamGasStream += 'BGCIN,'    
if options.beamGasODS != '':
    dictItem = MiscUtils.makeJediJobParam('${BGOIN}',options.beamGasODS,'input',hidden=True,
                                          expand=True,exclude='\.log\.tgz(\.\d+)*$',
                                          nFilesPerJob=options.nBeamGasO,useNumFilesAsRatio=True,
                                          reusableAtt=True)
    taskParamMap['jobParameters'] += dictItem
    inputMap['BGOIN'] = options.beamGasODS
    beamGasStream += 'BGOIN,'    
beamGasStream = beamGasStream[:-1]
if beamGasStream != '':
    dictItem = {'type':'constant',
                'value':'--beamGas "${{{0}/T}}"'.format(beamGasStream)
                }
    taskParamMap['jobParameters'] += [dictItem]


# output 
if options.addNthFieldOfInDSToLFN != '' or options.addNthFieldOfInFileToLFN != '':
    descriptionInLFN = '${MIDDLENAME}'
else:
    descriptionInLFN = ''
outMap,tmpParamList = AthenaUtils.convertConfToOutput(runConfig,options.extOutFile,options.outDS,
                                                      destination=options.destSE,spaceToken=options.spaceToken,
                                                      descriptionInLFN=descriptionInLFN,
                                                      allowNoOutput=options.allowNoOutput)
taskParamMap['jobParameters'] += [
    {'type':'constant',
     'value': '-o "%s" ' % outMap
     },
    ]
taskParamMap['jobParameters'] += tmpParamList 


# jobO parameter
if not options.trf:
    tmpJobO = jobO
    # replace full-path jobOs
    for tmpFullName,tmpLocalName in AthenaUtils.fullPathJobOs.iteritems():
        tmpJobO = re.sub(tmpFullName,tmpLocalName,tmpJobO)
    # modify one-liner for G4 random seeds
    if runConfig.other.G4RandomSeeds > 0:
        if options.singleLine != '':
            tmpJobO = re.sub('-c "%s" ' % options.singleLine,
                             '-c "%s;from G4AtlasApps.SimFlags import SimFlags;SimFlags.SeedsG4=${RNDMSEED}" ' \
                                 % options.singleLine,tmpJobO)
        else:
            tmpJobO = '-c "from G4AtlasApps.SimFlags import SimFlags;SimFlags.SeedsG4=${RNDMSEED}" ' + tmpJobO
        dictItem = {'type':'template',
                    'param_type':'number',
                    'value':'${RNDMSEED}',
                    'hidden':True,
                    'offset':runConfig.other.G4RandomSeeds,
                    }
        taskParamMap['jobParameters'] += [dictItem]
else:
    # replace parameters for TRF
    tmpJobO = jobO
    # output : basenames are in outMap['IROOT'] trough extOutFile
    tmpOutMap = []
    for tmpName,tmpLFN in outMap['IROOT']:
        tmpJobO = tmpJobO.replace('%OUT.' + tmpName,tmpName)
    # replace DBR
    tmpJobO = re.sub('%DB=[^ \'\";]+','${DBR}',tmpJobO)
# set jobO parameter
taskParamMap['jobParameters'] += [
    {'type':'constant',
     'value': '-j "',
     'padding':False,
     },
    ]
taskParamMap['jobParameters'] += PsubUtils.convertParamStrToJediParam(tmpJobO,inputMap,options.outDS[:-1],
                                                                      True,False,usePfnList)
taskParamMap['jobParameters'] += [
    {'type':'constant',
     'value': '"',
     },
    ]

# use local IO for trf
if (options.trf or runConfig.input.inBS) and not options.forceDirectIO:
    taskParamMap['useLocalIO'] = 1

# use AMI to get the number of events per file
if options.useAMIEventLevelSplit == True:
    taskParamMap['getNumEventsInMetadata'] = True

# force stage-in
if options.forceStaged:
    taskParamMap['useLocalIO'] = 1

# build step
if options.nobuild and not options.noCompile:
    pass
else:
    jobParameters = '-i ${IN} -o ${OUT} --sourceURL ${SURL} '
    # no compile
    if options.noCompile:
        jobParameters += "--noCompile "
    # use CMake
    if AthenaUtils.useCMake():
        jobParameters += "--useCMake "
    # debug parameters
    if options.queueData != '':
        jobParameters += "--overwriteQueuedata=%s " % options.queueData
    # set task param
    taskParamMap['buildSpec'] = {
        'prodSourceLabel':'panda',
        'archiveName':archiveName,
        'jobParameters':jobParameters,
        }
    if options.prodSourceLabel != '':
         taskParamMap['buildSpec']['prodSourceLabel'] = options.prodSourceLabel

# preprocessing step

# good run list
if options.goodRunListXML != '':
    jobParameters = "--goodRunListXML {0} ".format(options.goodRunListXML)
    if options.goodRunDataType != '':
        jobParameters += "--goodRunListDataType {0} ".format(options.goodRunDataType)
    if options.goodRunProdStep != '':
        jobParameters += "--goodRunListProdStep {0} ".format(options.goodRunProdStep)
    if options.goodRunListDS != '':
        jobParameters += "--goodRunListDS {0} ".format(options.goodRunListDS)
    jobParameters += "--sourceURL ${SURL} "
    # set task param
    taskParamMap['preproSpec'] = {
        'prodSourceLabel':'panda',
        'jobParameters':jobParameters,
        }
    if options.prodSourceLabel != '':
         taskParamMap['preproSpec']['prodSourceLabel'] = options.prodSourceLabel

# merging
if options.mergeOutput:
    jobParameters = '-r {0} '.format(runDir)
    if options.mergeScript != '':
        jobParameters += '-j "{0}" '.format(options.mergeScript)
    if not options.nobuild:
        jobParameters += '-l ${LIB} '
    else:
        jobParameters += '-a {0} '.format(archiveName)
        jobParameters += "--sourceURL ${SURL} "
    jobParameters += "--useAthenaPackages "
    if AthenaUtils.useCMake():
        jobParameters += "--useCMake "
    jobParameters += '${TRN_OUTPUT:OUTPUT} ${TRN_LOG:LOG}'
    taskParamMap['mergeSpec'] = {}
    taskParamMap['mergeSpec']['useLocalIO'] = 1
    taskParamMap['mergeSpec']['jobParameters'] = jobParameters
    taskParamMap['mergeOutput'] = True
    if options.nGBPerMergeJob > 0:
        taskParamMap['nGBPerMergeJob'] = options.nGBPerMergeJob

# check task parameters
PsubUtils.checkTaskParam(taskParamMap,options.unlimitNumOutputs)

if options.verbose:
    tmpLog.debug("==== taskParams ====")
    tmpKeys = taskParamMap.keys()
    tmpKeys.sort()
    for tmpKey in tmpKeys:
        print '%s : %s' % (tmpKey,taskParamMap[tmpKey])



#####################################################################
# submission

if options.nosubmit:
    # no submit
    pass
else:
    # upload proxy for glexec
    if Client.PandaSites.has_key(options.site):
        # delegation
        delResult = PsubUtils.uploadProxy(options.site,options.myproxy,gridPassPhrase,
                                          Client.PandaClouds[options.cloud]['pilotowners'],
                                          options.verbose)
        if not delResult:
            tmpLog.error("failed to upload proxy")
            sys.exit(EC_MyProxy)
    # submit task
    tmpLog.info("submit")
    status,tmpOut = Client.insertTaskParams(taskParamMap,options.verbose,True)
    # result
    if status != 0:
        tmpLog.error("task submission failed with {0}".format(status))
        sys.exit(EC_Submit)
    if tmpOut[0] in [0,3]:
        tmpLog.info(format(tmpOut[1]))
    else:
        tmpLog.error("task submission failed. {0}".format(tmpOut[1]))
        sys.exit(EC_Submit)

# go back to current dir
os.chdir(currentDir)
# succeeded
sys.exit(0)
